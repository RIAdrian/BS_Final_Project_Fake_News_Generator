{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04840d53-ad7f-43d0-9f03-88ce45ccfc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b780a471-df03-4e8f-8b1f-3c3f354fbc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"....\" # the path to the data base of contents for training \n",
    "data = pd.read_excel(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9418d1a2-46fc-476f-8ce7-e3efae324ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data['text'] = data['Titlu'] + \" [SEP] \" + data['Lead'] + \" [SEP] \" + data['Continut']\n",
    "texts = data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf55b1e-92dc-4f3f-b9e0-345e90e15874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_texts, val_texts = train_test_split(texts, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f1f4a3-d6c6-401e-8cc1-8cbab96425a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t\n",
    "with open('train_texts.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(train_texts))\n",
    "with open('val_texts.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(val_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b963fe8d-a267-4ad5-a7b8-de1ac7ebea6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading of GPT2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1889f9d-9f8c-4ed6-aa69-2a8745d05bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-processing Data\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train_texts.txt\",\n",
    "    block_size=128)\n",
    "\n",
    "val_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"val_texts.txt\",\n",
    "    block_size=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d759b37a-ba85-4ee0-a4e4-c3f302e11522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8edc7a2f-b608-4c4a-9788-646fc1254097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./...',  # Directory where model checkpoints and output files will be saved\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it already exists\n",
    "    num_train_epochs=15,  # Number of epochs to train the model\n",
    "    per_device_train_batch_size=4,  # Batch size for training on each device (e.g., GPU)\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation on each device\n",
    "    save_steps=10_000,  # Save the model checkpoint every 10,000 steps\n",
    "    save_total_limit=2,  # Keep only the 2 most recent checkpoints\n",
    "    prediction_loss_only=True,  # Return only the loss value during evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7686e30-0a0b-4929-98d5-8ce8ff386d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678341b-365e-4afd-8fd5-7201d0b3f88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0fb3b4-6fcb-47d3-9a68-b50a5c45f891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_path = './...' # the path for saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22b20f-1d00-475e-9e70-be66fe231b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87a88e-6424-4deb-8c57-48acac25a9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)  # Load the pre-trained tokenizer from the specified model path\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)  # Load the pre-trained model from the specified model path\n",
    "\n",
    "\n",
    "prompt_text = \"Lege noua financiara\"  # Prompt text to start generating the news article\n",
    "\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')  # Encode the prompt text into input IDs for the model\n",
    "\n",
    "generated_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=600,  # Maximum length of the generated text\n",
    "    num_beams=2,  # Enable beam search with 2 beams for more diverse and high-quality outputs\n",
    "    temperature=0.7,  # Adjust the temperature for controlling the randomness of predictions\n",
    "    no_repeat_ngram_size=2,  # Prevent repetition of 2-grams (2 consecutive words)\n",
    "    early_stopping=True,  # Stop generation early if all beams reach the end token\n",
    "    num_return_sequences=2,  # Number of generated sequences to return\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)  # Decode the first generated sequence\n",
    "print(\"Text generat:\")  # Print the header for the generated text\n",
    "print(generated_text)  # Print the generated text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70895b7b-1983-4e83-bb1c-5a5f8b3af790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
